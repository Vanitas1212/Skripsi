{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zberl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ PyTorch is using GPU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ PyTorch is using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"❌ PyTorch is NOT using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sentiment_distribution(dataset):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=dataset[\"Label\"], palette=\"coolwarm\")\n",
    "    plt.xticks(ticks=[0,1], labels=[\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Sentiment Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\zberl\\AppData\\Local\\Temp\\ipykernel_16752\\631407888.py:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  df = pd.read_excel(\"D:\\Skripsi-20250317T163434Z-001\\Skripsi\\skirpsi\\Code_Try\\How to normalize code mixing\\Translated_Mixed_unshufled_with-lexical.xlsx\", index_col=False)\n"
     ]
    }
   ],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = text.lower()  # Konversi ke lowercase\n",
    "    text = re.sub(r\"(?i)<\\s*username\\s*>\", \"\", text)  # Hapus tag username\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Hapus URL\n",
    "    text = re.sub(r\"\\s*@\\S+\", \"\", text) # Hapus mention\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Hapus karakter khusus (kecuali spasi)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Hapus spasi berlebih\n",
    "\n",
    "    # Hapus stopwords\n",
    "    # stop_words = set(stopwords.words(\"indonesian\"))\n",
    "    # words = text.split()\n",
    "    # filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # # Lakukan lemmatization pada setiap kata Inggris\n",
    "    # lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "    # # Gabungkan kembali kata-kata hasil stemming\n",
    "    # text = \" \".join(lemmatized_words)\n",
    "\n",
    "    # #Tokenization kalimat (pisah per kata)\n",
    "    # tokens = word_tokenize(text)\n",
    "    return text\n",
    "    # return \" \".join(filtered_words)\n",
    "\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"coolwarm\").generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def load_model():\n",
    "    # Load dataset\n",
    "    df = pd.read_excel(\"D:\\Skripsi-20250317T163434Z-001\\Skripsi\\skirpsi\\Code_Try\\How to normalize code mixing\\Translated_Mixed_unshufled_with-lexical.xlsx\", index_col=False)\n",
    "\n",
    "    # Hapus kolom Id\n",
    "    if \"Id\" in df.columns:\n",
    "        df.drop(columns=[\"Id\"], inplace=True)\n",
    "\n",
    "    # Konversi label sentimen ke numerik\n",
    "    # df[\"Label\"] = df[\"Label\"].map({\"negative\": 0, \"positive\": 1})\n",
    "    df[\"Label\"] = df[\"Label\"]\n",
    "\n",
    "\n",
    "    # Bersihkan teks komentar\n",
    "    df[\"Cleaned_Text\"] = df[\"Translated\"].apply(clean_text)\n",
    "    # df[\"Cleaned_Text\"] = df[\"Normalize\"]\n",
    "    # print(df[\"Label\"])\n",
    "    # Buat dataset final\n",
    "    dataset = pd.DataFrame({\"Text\": df[\"Cleaned_Text\"], \"Label\": df[\"Label\"]})\n",
    "    cyberbullying_text = \" \".join(df[df[\"Label\"] == 0][\"Cleaned_Text\"])\n",
    "    non_cyberbullying_text = \" \".join(df[df[\"Label\"] == 1][\"Cleaned_Text\"])\n",
    "    print(dataset)\n",
    "    generate_wordcloud(cyberbullying_text, \"Word Cloud - Cyberbullying Comments\")\n",
    "    generate_wordcloud(non_cyberbullying_text, \"Word Cloud - Non-Cyberbullying Comments\")\n",
    "\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indoBERT():\n",
    "    model_name = \"indobenchmark/indobert-base-p2\"\n",
    "    #declare nama model yang kita mau pakai\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    #ambil tokenizer dari indoBERT\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2, from_tf=True)\n",
    "    #ambil pretrained model nya, pakai num_labels=2 buat input berupa sequence\n",
    "    return tokenizer,model\n",
    "    #return tokenizer dan juga modlenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset, tokenizer):\n",
    "    X = list(dataset[\"Text\"])\n",
    "    y = list(dataset[\"Label\"])\n",
    "\n",
    "    # Tokenize the entire dataset at once\n",
    "    tokenized_data = tokenizer(X, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
    "        \"labels\": y\n",
    "    })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(eval_pred):\n",
    "    #Ini function buat evaluation metrice\n",
    "    # print(type(p))\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "    print(f\"Precision : {precision:.4f}\")\n",
    "    print(f\"Recall    : {recall:.4f}\")\n",
    "    print(f\"F1 Score  : {f1:.4f}\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(sample_text, model, tokenizer):\n",
    "    inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # Prediksi sentimen\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Konversi hasil ke probabilitas\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions = predictions.cpu().detach().numpy()\n",
    "\n",
    "    # Ambil kelas dengan probabilitas tertinggi\n",
    "    predicted_label = np.argmax(predictions)\n",
    "\n",
    "    # Tampilkan hasil prediksi\n",
    "    sentiment_label = \"Positive\" if predicted_label == 1 else \"Negative\"\n",
    "\n",
    "    # Print hasil prediksi\n",
    "    print(f\"Text: {sample_text}\")\n",
    "    print(f\"\\n📢 Predicted Sentiment: {sentiment_label}\")\n",
    "\n",
    "\n",
    "# def train_data(train_data,test_data,model,tokenizer):\n",
    "#     # args = TrainingArguments(\n",
    "#     #     output_dir=\"output\",\n",
    "#     #     num_train_epochs=1,\n",
    "#     #     per_device_train_batch_size=8\n",
    "#     # )\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=\"output\",\n",
    "#         num_train_epochs=1,  # Ubah jika perlu\n",
    "#         per_device_train_batch_size=8,\n",
    "#         per_device_eval_batch_size=8,\n",
    "#         # per_device_train_batch_size=4,\n",
    "#         # per_device_eval_batch_size=4,\n",
    "#         evaluation_strategy=\"epoch\",  # Evaluasi setiap epoch\n",
    "#         save_strategy=\"epoch\",\n",
    "#         logging_dir=\"./logs\",\n",
    "#         logging_steps=10,\n",
    "#         save_total_limit=2,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1\",\n",
    "#         greater_is_better=True,\n",
    "#         run_name=\"IndoBERT-CyberbullyingDetection\", #biar ga ada warning 'run_name' sama dgn 'output_dir\n",
    "#         report_to=\"none\" #biar ga minta API Key\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         train_dataset=train_data,\n",
    "#         eval_dataset=test_data,\n",
    "#         compute_metrics=evaluation_metrics\n",
    "#     )\n",
    "\n",
    "#     #Training Model\n",
    "#     trainer.train()\n",
    "\n",
    "#     #Evaluating Model\n",
    "#     eval_results = trainer.evaluate()\n",
    "#     print(\"\\n--- Evaluation Results ---\")\n",
    "#     for key, value in eval_results.items():\n",
    "#         print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "#     # np.set_printoptions(suppress=True)\n",
    "#     trainer.save_model('CustomModel')\n",
    "\n",
    "#     # Contoh prediksi setelah training\n",
    "#     sample_text = \"Bangga sama suami yg selalu ingat istri\"\n",
    "#     predict_sample(sample_text, model, tokenizer)\n",
    "\n",
    "#     # inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt')\n",
    "#     # outputs = model(**inputs)\n",
    "#     # predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#     # print(predictions)\n",
    "#     # predictions = predictions.cpu().detach().numpy()\n",
    "#     # predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_val_train(dataset, model, tokenizer, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f\"\\n========== Fold {fold + 1} ==========\")\n",
    "\n",
    "        # Split dataset into train and validation sets\n",
    "        train_subset = dataset.select(train_idx)\n",
    "        val_subset = dataset.select(val_idx)\n",
    "\n",
    "        # Training arguments\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"output_fold{fold + 1}\",\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=f\"./logs_fold{fold + 1}\",\n",
    "            logging_steps=10,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            run_name=f\"IndoBERT-CyberbullyingDetection-Fold{fold+1}\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_subset,\n",
    "            eval_dataset=val_subset,\n",
    "            compute_metrics=evaluation_metrics\n",
    "        )\n",
    "\n",
    "        # Training\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluation\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        for key, value in eval_results.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "        # Save the model for this fold\n",
    "        trainer.save_model(f'CustomModel_Fold{fold+1}')\n",
    "\n",
    "    # Aggregate Results\n",
    "    print(\"\\n===== Cross-Validation Results =====\")\n",
    "    avg_results = {key: np.mean([fold[key] for fold in fold_results]) for key in fold_results[0]}\n",
    "    for key, value in avg_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    return avg_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_model()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# visualisasi proporsi jumlah data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     visualize_sentiment_distribution(dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = load_model()\n",
    "\n",
    "    # visualisasi proporsi jumlah data\n",
    "    visualize_sentiment_distribution(dataset)\n",
    "\n",
    "    tokenizer,model = load_indoBERT()\n",
    "    # sample = [\"Bodoh\"]\n",
    "    # nyehe = tokenizer(sample, padding=True, truncation=True, max_length=512)\n",
    "    # print(nyehe)\n",
    "    # X_train, X_val, y_train, y_val = prepare_data(dataset,tokenizer)\n",
    "    data = prepare_data(dataset,tokenizer)\n",
    "    average = cross_val_train(data,model,tokenizer)\n",
    "    print(average)\n",
    "    # train_dataset = Dataset(X_train,y_train)\n",
    "    # test_dataset = Dataset(X_val,y_val)\n",
    "    # train_data(train_dataset,test_dataset,model,tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "    # model_2 = BertForSequenceClassification.from_pretrained(\"CustomModel\")\n",
    "    # text = \"goblok lu\"\n",
    "    # inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt')\n",
    "    # outputs = model_2(**inputs)\n",
    "    # predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # predictions = predictions.cpu().detach().numpy()\n",
    "    # predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
